{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "condo_data_webscraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfch1999/Condos.ca-Prediction/blob/main/condo_data_webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbr_pjRmiylz"
      },
      "source": [
        "Web Scraping Condos.ca with Scrapy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZBIUfoMOqLr7",
        "outputId": "962ef770-e71d-4078-b4bf-b4d110272d51"
      },
      "source": [
        "# Settings for notebook\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "# Show Python version\n",
        "import platform\n",
        "platform.python_version()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.7.12'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORhTNtHd1Jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3251db-5a5f-4e27-cfcc-934d4e0cd878"
      },
      "source": [
        "# Import packages\n",
        "try:\n",
        "    import scrapy\n",
        "except:\n",
        "    !pip install scrapy\n",
        "    import scrapy\n",
        "\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "import pandas as pd\n",
        "from multiprocessing import Process, Queue\n",
        "from twisted.internet import reactor\n",
        "from scrapy.http import FormRequest\n",
        "!pip install loginform\n",
        "import loginform\n",
        "from loginform import fill_login_form\n",
        "import re\n",
        "!pip install html2text\n",
        "import html2text\n",
        "\n",
        "# start scrapy project\n",
        "!scrapy startproject condos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: loginform in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from loginform) (4.2.6)\n",
            "Collecting html2text\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2020.1.16\n",
            "Error: scrapy.cfg already exists in /content/condos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeZRttLUBaEB"
      },
      "source": [
        "# code to be able to run the crawler twice without restarting the runtime (Note: I found this code on stackoverflow)\n",
        "def run_spider(spider):\n",
        "    def f(q):\n",
        "        try:\n",
        "            runner = scrapy.crawler.CrawlerRunner()\n",
        "            deferred = runner.crawl(spider)\n",
        "            deferred.addBoth(lambda _: reactor.stop())\n",
        "            reactor.run()\n",
        "            q.put(None)\n",
        "        except Exception as e:\n",
        "            q.put(e)\n",
        "\n",
        "    q = Queue()\n",
        "    p = Process(target=f, args=(q,))\n",
        "    p.start()\n",
        "    result = q.get()\n",
        "    p.join()\n",
        "\n",
        "    if result is not None:\n",
        "        raise result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEQ4PE4QNJeX"
      },
      "source": [
        "# write code to scrape multiple condo's from the start page\n",
        "class CondoSpider(scrapy.Spider): \n",
        "  name = \"condo\"\n",
        "  start_urls = [\"https://condos.ca/toronto\"]\n",
        "  BASE_URL = \"https://condos.ca\"\n",
        "\n",
        "  login_url = \"https://api.condos.ca/login\"\n",
        "  login_user = 'cjlemesurier@gmail.com'\n",
        "  login_password = 'Lemesurier1!'\n",
        " \n",
        "  def start_requests(self):\n",
        "      # start by sending a first request to login page\n",
        "      yield scrapy.Request(self.login_url, self.parse)\n",
        "\n",
        "  def parse(self, response):\n",
        "      # got the login page, let's fill the login form...\n",
        "      data, url, method = fill_login_form(response.url, response.body,\n",
        "                                            self.login_user, self.login_password)\n",
        "      # ... and send a request with our login data\n",
        "      return scrapy.FormRequest(url, formdata=dict(data),\n",
        "                           method=method, callback=self.parse_main)\n",
        "\n",
        "  def parse_main(self, response):\n",
        "    # set up standard headers for csv file\n",
        "    features = ['Actual size', 'Exposure', 'Maintenance fees', 'Possession', 'Age of building',\n",
        "       'Outdoor space', 'Price/sqm', 'Locker', 'Heating type:',\n",
        "       'Parking type:', 'Property type:', 'Area:', 'Ensuite laundry:',\n",
        "       'Parking:', 'Corp #:', 'Size:', 'Show all', 'Bed', 'Bath', 'Parking',\n",
        "       'price', 'Broker', 'MLS Number', 'address', 'amenities', 'included_in_maintenance_fees']\n",
        "    header_df = pd.DataFrame(columns = features)\n",
        "    header_df.to_csv(\"condoData.csv\",sep=\",\", mode='w', header=True)\n",
        "\n",
        "    # cycle through the 75 pages of toronto condo listings\n",
        "    for pageNumber in range(1,76):\n",
        "      nextPageLink = self.BASE_URL + \"/toronto?page=\" + str(pageNumber)\n",
        "      yield scrapy.Request(nextPageLink, callback=self.parse_page)\n",
        "\n",
        "  def parse_page(self, response):\n",
        "    # get links to all condo listings on the current page\n",
        "    condoLinks = response.xpath(\"//div[contains(@class,'ListingPreview')]//a/@href\").getall()\n",
        "\n",
        "    # cycle through each link and call the parse_child function    \n",
        "    for link in condoLinks:\n",
        "      fullLink = self.BASE_URL + link\n",
        "      yield scrapy.Request(fullLink, callback=self.parse_child, \n",
        "                           headers={'authorization':'Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwOi8vYXBpLmNvbmRvcy5jYS92MS91c2Vycy9yZWZyZXNoIiwiaWF0IjoxNjM3MDIzNTY5LCJleHAiOjE2MzcwMjcxNjksIm5iZiI6MTYzNzAyMzU2OSwianRpIjoiYzMyMmZxQUliczNMOU80ZyIsInN1YiI6MTI2ODgxMiwicHJ2IjoiODdlMGFmMWVmOWZkMTU4MTJmZGVjOTcxNTNhMTRlMGIwNDc1NDZhYSIsImlkIjoxMjY4ODEyLCJlbWFpbCI6InNvcjEwMDEwMEB5YWhvby5jb20iLCJmaXJzdF9uYW1lIjoiU29yb3VzaCIsImxhc3RfbmFtZSI6IkZhcmdoYWRhbmkiLCJwaG9uZSI6IjQzNzk3MTcwNzAiLCJwaG9uZV9jb3VudHJ5X2NvZGUiOiIrMSIsInBob25lX2xvY2F0aW9uIjpudWxsLCJoYXNQYXNzd29yZCI6MSwic3RhdGVkX3ByaWNlX21pbiI6bnVsbCwic3RhdGVkX3ByaWNlX21heCI6bnVsbCwidGV4dF92YWxpZGF0ZWQiOjEsImFnZW50X2lkIjo0NTUsImVtYWlsX3ZlcmlmaWVkIjoxLCJyZWZlcnJhbF9jb2RlIjpudWxsLCJyb2xlcyI6W3sibmFtZSI6InJlbnRhbF9sZWFkIn0seyJuYW1lIjoiYnV5ZXJfbGVhZCJ9XSwibWVudV9saW5rcyI6W10sImFnZW50Ijp7ImlkIjo0NTUsImZpcnN0bmFtZSI6IkpvcmRhbiIsImZ1bGxuYW1lIjoiSm9yZGFuIEhhcnJpbmd0b24iLCJ0aXRsZSI6IlNhbGVzIFJlcHJlc2VudGF0aXZlIiwiYnJva2VyYWdlX25hbWUiOiJQcm9wZXJ0eS5jYSBJbmMuIiwiY2VsbCI6IisxNjQ3MzY1NTcxOCIsImVtYWlsIjoiam9yZGFuaGFycmluZ3RvbkBwcm9wZXJ0eS5jYSIsInByb2ZpbGVfaW1hZ2UiOnsiaWQiOjMyMzAyLCJvcmlnaW5hbF9wYXRoIjoiaHR0cHM6Ly9zaGFyZWQtczMucHJvcGVydHkuY2EvcHVibGljL2ltYWdlcy9hZ2VudHMvNDU1LzQ1NS1vcmlnaW5hbC02MDNlYmJlMmI0MGMzMi4yODEzNDU0Mi5qcGciLCJub3JtYWxfcGF0aCI6Imh0dHBzOi8vc2hhcmVkLXMzLnByb3BlcnR5LmNhL3B1YmxpYy9pbWFnZXMvYWdlbnRzLzQ1NS80NTUtbm9ybWFsLTYwM2ViYmUyYjQwYzMyLjI4MTM0NTQyLmpwZyIsInRodW1iX3BhdGgiOiJodHRwczovL3NoYXJlZC1zMy5wcm9wZXJ0eS5jYS9wdWJsaWMvaW1hZ2VzL2FnZW50cy80NTUvNDU1LXRodW1iLTYwM2ViYmUyYjQwYzMyLjI4MTM0NTQyLmpwZyIsInNtYWxsX3BhdGgiOiJodHRwczovL3NoYXJlZC1zMy5wcm9wZXJ0eS5jYS9wdWJsaWMvaW1hZ2VzL2FnZW50cy80NTUvNDU1LXNtYWxsLTYwM2ViYmUyYjQwYzMyLjI4MTM0NTQyLmpwZyJ9LCJ1cmwiOiJjb25kby1wcm9zL2pvcmRhbi1oYXJyaW5ndG9uIn19.KpLr7yQm10n1XMqUFIqPG4MLh1L9hl-bssoYlSEAWiA'})\n",
        "\n",
        "  def parse_child(self, response):\n",
        "    # pull the text from all divs (including children) with class name \"ListingDetailKeyContainer\"\n",
        "    keyListingDetails = response.xpath(\"//div[contains(@class,'ListingDetailKeyContainer')]//text()\").getall()\n",
        "\n",
        "    # build a dataframe - the list alternates between features names and the value\n",
        "    i=1\n",
        "    features = []\n",
        "    values = []\n",
        "    for element in keyListingDetails:\n",
        "      if i%2==0:\n",
        "        values.append(element)\n",
        "      else:\n",
        "        features.append(element)\n",
        "      i=i+1\n",
        "    listingDetails_df = pd.DataFrame(values, index=features).transpose()\n",
        "\n",
        "    # scrape bed, bath, parking numbers\n",
        "    condoRoomCounts = response.xpath(\"//div[contains(@class,'Details')]//text()\").getall()\n",
        "    if condoRoomCounts[0]==\"Studio\":\n",
        "      listingDetails_df['Bed'] = condoRoomCounts[0]\n",
        "      listingDetails_df['Bath'] = condoRoomCounts[2]\n",
        "      listingDetails_df['Parking'] = condoRoomCounts[5]\n",
        "    else:\n",
        "      listingDetails_df['Bed'] = condoRoomCounts[0]\n",
        "      listingDetails_df['Bath'] = condoRoomCounts[3]\n",
        "      listingDetails_df['Parking'] = condoRoomCounts[6]\n",
        "\n",
        "    # scrape price\n",
        "    condoPrice = response.xpath(\"//div[contains(@class,'Price')]//text()\").get()\n",
        "    listingDetails_df['price'] = condoPrice\n",
        "\n",
        "    # scrape Brokerage and MLS number\n",
        "    BrokMLSInfo = response.xpath(\"//div[contains(@class,'BrokrageAndMLSCont')]//text()\").getall()\n",
        "    listingDetails_df['Broker'] = BrokMLSInfo[1]\n",
        "    listingDetails_df['MLS Number'] = BrokMLSInfo[5]\n",
        "\n",
        "    # scrape address\n",
        "    condoAddress = response.xpath(\"//h1[contains(@class,'Title')]//text()\").get()\n",
        "    listingDetails_df['address'] = condoAddress\n",
        "\n",
        "    # scrape acutal size from about section\n",
        "    paragraph = response.xpath(\"//div[contains(@class,'MaxWidthGridV2')]//p//text()\").get()\n",
        "    end = paragraph.find(\" sqft\")\n",
        "    if end>=0:\n",
        "      # cut off everything afte and including \"sqft\" - we know the number for acutal size always comes right before \"sqft\"\n",
        "      paragraph_cut = paragraph[0:end]\n",
        "      # find the last space in the cut off paragraph\n",
        "      start = paragraph_cut.rfind(\" \") + 1\n",
        "      # actual size will be everythin after the last space (ie just the number)\n",
        "      actual_size = paragraph_cut[start:]\n",
        "      listingDetails_df['Actual size'] = actual_size\n",
        "\n",
        "\n",
        "    # scrape the bed and bath counts from the about section, if they are NA \n",
        "    if listingDetails_df['Bed'][0] == \"NA\":\n",
        "      end_bed = paragraph.find(\" bed\")\n",
        "      if end_bed>=0:\n",
        "        paragraph_cut = paragraph[0:end_bed]\n",
        "        start_bed = paragraph_cut.rfind(\" \") + 1\n",
        "        number_of_beds = paragraph_cut[start_bed:]\n",
        "        listingDetails_df['Bed'] = number_of_beds\n",
        "    \n",
        "    if listingDetails_df['Bath'][0] == \"NA\":\n",
        "      end_bath = paragraph.find(\" bath\")\n",
        "      if end_bath>=0:\n",
        "        paragraph_cut = paragraph[0:end_bath]\n",
        "        start_bath = paragraph_cut.rfind(\" \") + 1\n",
        "        number_of_baths = paragraph_cut[start_bath:]\n",
        "        listingDetails_df['Bath'] = number_of_baths\n",
        "\n",
        "    # scrape amenities\n",
        "    amenities_data = response.xpath(\"//div[contains(@class,'AmenityIconContainer')]\")\n",
        "    amenities_raw = amenities_data[0].xpath(\".//text()\").getall()\n",
        "    utilities_raw = amenities_data[1].xpath(\".//text()\").getall()\n",
        "\n",
        "    amenities = []\n",
        "    for i in range(len(amenities_raw)):\n",
        "      if i%3 == 1:\n",
        "        amenities.append(amenities_raw[i])\n",
        "    \n",
        "    amenities_str = ', '.join(str(e) for e in amenities)\n",
        "    listingDetails_df['amenities'] = amenities_str\n",
        "\n",
        "    utilities = []\n",
        "    for i in range(len(utilities_raw)):\n",
        "      if i%3 == 1:\n",
        "        utilities.append(utilities_raw[i])\n",
        "    \n",
        "    utilities_str = ', '.join(str(e) for e in utilities)\n",
        "    listingDetails_df['included_in_maintenance_fees'] = utilities_str\n",
        "\n",
        "    # put fetaures in a standard order, so we can write to a csv\n",
        "    features = ['Actual size', 'Exposure', 'Maintenance fees', 'Possession', 'Age of building',\n",
        "       'Outdoor space', 'Price/sqm', 'Locker', 'Heating type:',\n",
        "       'Parking type:', 'Property type:', 'Area:', 'Ensuite laundry:',\n",
        "       'Parking:', 'Corp #:', 'Size:', 'Show all', 'Bed', 'Bath', 'Parking',\n",
        "       'price', 'Broker', 'MLS Number', 'address', 'amenities', 'included_in_maintenance_fees']\n",
        "    standard_df = pd.DataFrame(columns = features)\n",
        "    standard_df = standard_df.append([listingDetails_df])\n",
        "\n",
        "    yield standard_df.to_csv(\"condoData.csv\",sep=\",\", mode='a', header=False)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5WtVIMSqDzF"
      },
      "source": [
        "# call the function\n",
        "run_spider(CondoSpider)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teaGtcp4-4H_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}